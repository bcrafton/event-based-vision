
a smaller network with random weights actually works well
big problem is we are double boxing literally everything

~0.5 loss was corresponding to pretty much perfect accuracy, BUT we are double boxing everything.

something is up with obj/no_obj tho
because its usually a 50-50 split
and the loss is 0.5
AND we have double boxes.

total: 1392, rate: 33.584092, loss 0.495076 (7 0 53 38 0)

------

what about just a single box ? 

------

there is def something wrong with calc_map i think.

------

dataset_visualization literally loads every file in the list
just make it not do that 
and we shud be good

------

overfitting.

> starting with actual DNN.
  > not training it tranferred layers

> image augmentation

------

wrote a bunch of notes on loader changes:

basically:
dataset, load

load diverged from dataset at ' update design notes '
meaning only real changes are:
https://github.com/bcrafton/event-based-vision/commits/load
https://github.com/bcrafton/event-based-vision/commit/e01c12188f519fc38a4e7148023df97c8a7fdfed#diff-e44f4a60e89e820dde9bb27afa634965

------

> load.py
> train.py
> dataset_visualization.py

------

WARNING:tensorflow:5 out of the last 11 calls to <function gradients at 0x7fef11ecf2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.

https://www.tensorflow.org/guide/function
https://www.tensorflow.org/guide/function#python_or_tensor_args

------

> https://www.tensorflow.org/guide/data#consuming_tfrecord_data
> https://www.tensorflow.org/guide/data#consuming_sets_of_files

------

so we really want to get that perf boost we know exists.

------

> tensor slicing ? 
> .numpy ()
> detection dimension ? 
  > can we hard code this to 8 ? 

------
  
so with inference, we are only getting ~120 FPS.
that just aint right.

this is a reduced ResNet.

------

this should be fine:
https://www.tensorflow.org/guide/data#consuming_sets_of_files

how will we batchify things tho ? 
now we are using a fixed det size
so that wont be too bad i guess

yeah actually we can make this part work then.

------

so new problem i dealing with labels
we have too many labels
we will need to stack them somehow so we only get a single label

or maybe it does allow for multiple returns and it will handle the stacking for u idk.

------

alright, so just make the tfrecords work.
hopefully allows us to go much faster.

then try transforms and shit on the data.

------

>>> holy shit it was just making this tf.function

@tf.function(experimental_relax_shapes=False)
def extract_fn(record):

------

> put the old stuff back in.
> image transforms to make generalization work.
  > random crop --- 224,224
  > resize to larger image, then crop.

------

> OH shit - dont forget window size is changed. they use width=304.

> we should be saving the original label inside of the tfrecord ... so we dont have to recreate it like this ...

------

>> make sure (truth, pred) in function calls

------

Traceback (most recent call last):
  File "/home/bcrafton3/python_environment/py3/lib/python3.6/site-packages/tensorflow/python/eager/context.py", line 1986, in execution_mode
    yield
  File "/home/bcrafton3/python_environment/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 655, in _next_internal
    output_shapes=self._flat_output_shapes)
  File "/home/bcrafton3/python_environment/py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 2363, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File "/home/bcrafton3/python_environment/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 6653, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 12 [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 186, in <module>
    for (x, y) in dataset:
  File "/home/bcrafton3/python_environment/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 631, in __next__
    return self.next()
  File "/home/bcrafton3/python_environment/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 670, in next
    return self._next_internal()
  File "/home/bcrafton3/python_environment/py3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 661, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File "/usr/lib/python3.6/contextlib.py", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File "/home/bcrafton3/python_environment/py3/lib/python3.6/site-packages/tensorflow/python/eager/context.py", line 1989, in execution_mode
    executor_new.wait()
  File "/home/bcrafton3/python_environment/py3/lib/python3.6/site-packages/tensorflow/python/eager/executor.py", line 67, in wait
    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 12

>> this is unfortunate.
> especially because it was actually doing well.

> not sure how the others are still running if this threw a corrupted record error.

------

we can probably build some [loss, mAP] table on what to expect given some loss.

rerunning from checkpoint ...

------

(1)
AHHH - so loss issue def because of different padding used between the 2 models.
or atleast thats part of it.

(2) 
we can run multi gpu training with keras which is sick.

(3)
https://github.com/FMsunyh/keras-yolo/blob/master/core/layers/_losses.py
trying to figure out why keras cannot train our model.
loss function autograd problems ? 

0.145669 (7 2 57 31 1)
> need to get what they usually look like 
> something like this

what about those errors right at the start of the program ? 
experimental_relax_shapes=True or something ??

stop using multi gpu ? 
> wud hopefully make it easier to debug.

------

really just thinking it has to do with K vs TF.
but dont want to invest more time in this
feeling like waste of time ...
do quantum or ADC 

------

dont think answer is keras.mean() or w.e.
isnt keras wrapping tf ? 
and keras-yolov1 ... that github repo ... uses tf 
so w.e. 










