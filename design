
a smaller network with random weights actually works well
big problem is we are double boxing literally everything

~0.5 loss was corresponding to pretty much perfect accuracy, BUT we are double boxing everything.

something is up with obj/no_obj tho
because its usually a 50-50 split
and the loss is 0.5
AND we have double boxes.

total: 1392, rate: 33.584092, loss 0.495076 (7 0 53 38 0)

------

what about just a single box ? 

------

there is def something wrong with calc_map i think.

------

dataset_visualization literally loads every file in the list
just make it not do that 
and we shud be good

------

overfitting.

> starting with actual DNN.
  > not training it tranferred layers

> image augmentation

------

wrote a bunch of notes on loader changes:

basically:
dataset, load

load diverged from dataset at ' update design notes '
meaning only real changes are:
https://github.com/bcrafton/event-based-vision/commits/load
https://github.com/bcrafton/event-based-vision/commit/e01c12188f519fc38a4e7148023df97c8a7fdfed#diff-e44f4a60e89e820dde9bb27afa634965

------

> load.py
> train.py
> dataset_visualization.py

------

WARNING:tensorflow:5 out of the last 11 calls to <function gradients at 0x7fef11ecf2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.

https://www.tensorflow.org/guide/function
https://www.tensorflow.org/guide/function#python_or_tensor_args

------

> https://www.tensorflow.org/guide/data#consuming_tfrecord_data
> https://www.tensorflow.org/guide/data#consuming_sets_of_files

------

so we really want to get that perf boost we know exists.

------

> tensor slicing ? 
> .numpy ()
> detection dimension ? 
  > can we hard code this to 8 ? 

------
  
so with inference, we are only getting ~120 FPS.
that just aint right.

this is a reduced ResNet.

------

this should be fine:
https://www.tensorflow.org/guide/data#consuming_sets_of_files

how will we batchify things tho ? 
now we are using a fixed det size
so that wont be too bad i guess

yeah actually we can make this part work then.

------

so new problem i dealing with labels
we have too many labels
we will need to stack them somehow so we only get a single label

or maybe it does allow for multiple returns and it will handle the stacking for u idk.

------

alright, so just make the tfrecords work.
hopefully allows us to go much faster.

then try transforms and shit on the data.

------

>>> holy shit it was just making this tf.function

@tf.function(experimental_relax_shapes=False)
def extract_fn(record):

------

> put the old stuff back in.
> image transforms to make generalization work.
  > random crop --- 224,224
  > resize to larger image, then crop.

------

> OH shit - dont forget window size is changed. they use width=304.

> we should be saving the original label inside of the tfrecord ... so we dont have to recreate it like this ...

------





